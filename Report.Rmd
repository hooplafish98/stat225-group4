---
title: "Predicting Player Minutes based on Per 36 Rate Statistics"
author: "Group 4: Vignesh Mahalingam, Brandon Wang, Arjie Nanda"
date: ""
output: 
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---

```{r, setup, include=FALSE}
require(mosaic)   # Load additional packages here 
library(readr)
library(boot) #for bootstrap
library(Rfit)
library(ggplot2)
library(gam)
library(splines)
library(gridExtra)
knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```
```{r, include=FALSE}
#loaded the data here... not sure if we need to show this on a slide or not

#Fit setup
#Fit statistics for OLS

fit_ols = function(model) {
  yy = model$residuals + model$fitted.values
  rsq = 1- sum(model$residuals^2)/sum((yy- mean(yy))^2)
  nn =length(yy)
  adjrsq = 1-(1-rsq)*((nn-1)/(nn- length(model$coefficients)))
  propL1 = 1- sum(abs(model$residuals))/sum(abs(yy- mean(yy)))
  
  return(cbind(rsq = rsq, adjrsq = adjrsq, propL1 = propL1))
  }

#Fit statistics for JHM

fit_jhm = function(model) {
  rsq = 1- sum(model$residuals^2)/sum((model$y- mean(model$y))^2)
  nn =length(model$y)
  adjrsq = 1-(1-rsq)*((nn-1)/(nn- length(model$coefficients)))
  propL1 = 1- sum(abs(model$residuals))/sum(abs(model$y- mean(model$y)))
  
  return(cbind(rsq = rsq, adjrsq = adjrsq, propL1 = propL1))
  }

#Fit statistics for GAM

fit_gam = function(model) {
  rsq = 1-model$deviance/model$null.deviance
  adjrsq = 1-(1-rsq)*(model$df.null/model$df.residual)
  propL1 = 1- sum(abs(model$residuals))/sum(abs(model$y- mean(model$y)))
  
  return(cbind(rsq = rsq, adjrsq = adjrsq, propL1 = propL1))}

#CV setup

#General fit statistics

fit_gen = function(y, res, df){
  rsq = 1- sum(res^2)/sum((y- mean(y))^2)
  nn =length(y)
  adjrsq = 1-(1-rsq)*((nn-1)/(nn-df))
  propL1 = 1- sum(abs(res))/sum(abs(y- mean(y)))
  
  return(cbind(rsq = rsq, adjrsq = adjrsq, propL1 = propL1))
  }

#My cross-validation function for this project

cv_rmc = function(dat, ols_mod, jhm_mod, gam_mod, k = 5, m = 10){
  
  #(Some) error checking
  
  if(class(ols_mod)!="lm")stop('ols_mod should come from the lm() function')
  if(class(jhm_mod)!="rfit")stop('jhm_mod should come from the rfit() function')
  if(class(gam_mod)[1]!="Gam")stop('gam_mod should come from the gam() function')
  #Create model call character strings with subsetted data; uses stringr f()s
  dat.name =paste0("data = ",deparse(substitute(dat)))
  ols_call =capture.output(ols_mod$call)
  ols_call =str_replace(ols_call, dat.name, "data = dat[-part[[i]], ]")
  jhm_call =capture.output(jhm_mod$call)
  jhm_call =str_replace(jhm_call, dat.name, "data = dat[-part[[i]], ]")
  gam_call =paste(str_trim(capture.output(gam_mod$call)), sep="", collapse="")
  gam_call =str_replace(gam_call, dat.name, "data = dat[-part[[i]], ]")
  #Set up objects
  ols_fit =matrix(nrow = m, ncol = 3)
  jhm_fit = ols_fit; gam_fit = ols_fit
  yy = jhm_mod$y
  nn =dim(as.data.frame(dat))[1]
  oos_lmres =vector(length = nn)
  oos_jhres = oos_lmres; oos_gares = oos_lmres
  df_ols =length(ols_mod$coefficients)
  df_jhm =length(jhm_mod$coefficients)
  df_gam = nn-gam_mod$df.residual
  
  #Repeat k-fold cross-validation m times
  for(j in 1:m) {
    #Split data into k equal-ish parts, with random indices
    part =suppressWarnings(split(sample(nn), 1:k))
    #Execute model calls for all k folds; %*% is matrix multiplication
    for(i in 1:k){
      lm_mod =eval(parse(text = ols_call))
      pred =predict(object = lm_mod, newdata = dat[part[[i]],])
      oos_lmres[part[[i]]] = yy[part[[i]]]-pred
      jh_mod =eval(parse(text = jhm_call))
      subdat =select(.data = dat,colnames(jh_mod$x)[-1])[part[[i]],]
      subdat =cbind(1,as.matrix.data.frame(subdat))
      pred = subdat%*%jh_mod$coefficients
      oos_jhres[part[[i]]] = yy[part[[i]]]-pred
      
      ga_mod =eval(parse(text = gam_call))
      pred =predict(object = ga_mod, newdata = dat[part[[i]],])
      oos_gares[part[[i]]] = yy[part[[i]]]-pred
      }
    ols_fit[j, ] =fit_gen(y = yy, res = oos_lmres, df = df_ols)
    jhm_fit[j, ] =fit_gen(y = yy, res = oos_jhres, df = df_jhm)
    gam_fit[j, ] =fit_gen(y = yy, res = oos_gares, df = df_gam)}
  #Manage output -- average fit statistics
  outtie = rbind(colMeans(ols_fit),colMeans(jhm_fit),colMeans(gam_fit))
  colnames(outtie) = paste0("cv.",colnames(fit_ols(lm_mod)))
  row.names(outtie) =c("OLS", "JHM", "GAM")
  
  return(outtie)
  }

#Setup for everything else

URL = "https://www.amherst.edu/system/files/media/SP20_S06_Group4_data.csv"
nbaper36 <- read_csv(URL) #reads in csv #reads in csv
totplayers = as.list(nbaper36 %>% filter(Tm == "TOT") %>% select(Player))#list of players on multiple teams in season
nbaper36 = nbaper36 %>% mutate(MP = MP/G) %>% filter(G > 41) %>% filter(!(Player %in% totplayers$Player & Tm != "TOT")) #mutates MP to reflect minutes played per game, filters for players who played >41 games, and removes player entries that are not season totals
nbaper36select = select(nbaper36, Player, MP, TRB, AST, TOV, FTA, PTS, 'FG%') %>% rename(FG = 'FG%')
```


In this report, we plan creating models for predicting minutes played through the other types of basketball statistics. Minutes played is a valuable stat because it keeps track of how often a player is on the court. More minutes played gives players more time to contribute to the game. We will use a combination of both counting stats and rate stats. We hypothesize that the counting stats will be positively correlated with minutes played, while the rate stats should have no relationship. It is certainly possible that players who have higher rate stats, like three point percentage, are more valuable and will play more minutes as a result. On the other hand, players who play fewer minutes could have better rates due to the increased variability in a smaller sample. We are using the per 36 player data from the NBA’s 2018-19 season, which can be found at https://www.basketball-reference.com/leagues/NBA_2019_per_minute.html . This data is taken from the NBA’s own statistics page, and is a widely used industry source. Per 36 refers to adjusting the player’s statistics to project what their stats would be if the player played 36 minutes per game. We will be using the per 36 data as it provides a good insight into a player’s productivity without interference from the number of minutes played by the player.  The individual population units are players with a minimum of 41 games played, which is half the season. We’re trying to generalize this to future NBA seasons, as a method of estimating the average number of minutes played by a given player. We believe there are around 240 such players in the league. We also decided to restrict our predictor variables to points, rebounds, assists, turnovers, free throws attempted, and field goal percentage, as these are common stats used in player evaluation. 

##Exploratory Analysis

We first decided to do some exploratory data analysis on our data. For the initial univariate data exploration, we settled on using Kernel Density Estimations (KDEs) with an Epanechnikov kernel and Sheather-Jones bandwidth. We felt these KDEs would be most appropriate as they hold no assumptions about the underlying distribution of the data. 

```{r kde}
kde1 = ggplot(data = nbaper36select, aes(MP)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde2 = ggplot(data = nbaper36select, aes(TRB)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde3 = ggplot(data = nbaper36select, aes(AST)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde4 = ggplot(data = nbaper36select, aes(TOV)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde5 = ggplot(data = nbaper36select, aes(FTA)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde6 = ggplot(data = nbaper36select, aes(PTS)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde7 = ggplot(data = nbaper36select, aes(FG)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)

grid.arrange(kde1, kde2, kde3, kde4, kde5, kde6, kde7, ncol = 3)
```

From the KDEs we can see a marked difference in the KDEs of the predictor variables and the response variables. The predictor variables have distributions with early peaks and long tails, illustrating that the bulk of NBA players have average stats, with only a few players having lower rate statistics and relatively more players having higher rate statistics as compared to the mean. The `MP` KDE shows a much different pattern, instead having a slightly bimodal distribution that corresponds with the minutes played for bench players and starters.

We also created a series of scatterplots comparing `MP` with each predictor variable. 
```{r}
<<<<<<< HEAD
URL = "https://www.amherst.edu/system/files/media/SP20_S06_Group4_data.csv"
nbaper36 <- read_csv(URL) #reads in csv #reads in csv
totplayers = as.list(nbaper36 %>% filter(Tm == "TOT") %>% select(Player))#list of players on multiple teams in season
nbaper36 = nbaper36 %>% mutate(MP = MP/G) %>% filter(G > 41) %>% filter(!(Player %in% totplayers$Player & Tm != "TOT")) #mutates MP to reflect minutes played per game, filters for players who played >41 games, and removes player entries that are not season totals
nbaper36select = select(nbaper36, Player, MP, TRB, AST, TOV, FTA, PTS, 'FG%') %>% rename(FG = 'FG%')

=======
splot1 <- ggplot(data = nbaper36select, aes(x = TRB, y = MP)) + 
  geom_point()

splot2 <- ggplot(data = nbaper36select, aes(x = AST, y = MP)) + 
  geom_point()
splot3 <- ggplot(data = nbaper36select, aes(x = TOV, y = MP)) + 
  geom_point()
splot4 <- ggplot(data = nbaper36select, aes(x = FTA, y = MP)) + 
  geom_point()
splot5 <- ggplot(data = nbaper36select, aes(x = PTS, y = MP)) + 
  geom_point()
splot6 <- ggplot(data = nbaper36select, aes(x = FG, y = MP)) + 
  geom_point()

grid.arrange(splot1, splot2, splot3, splot4, splot5, splot6, ncol = 2)
```
From this we can see that some of the variables have a positive relationship with `MP`, while some have a relationship that is more variable.


## OLM and Bootstrap
>>>>>>> 00e37acb971d56c45256ce402fa3702e22ca4bab

```{r}
#OLM
model_olm = lm(data = nbaper36, MP ~ TRB + BLK + TOV + FTA + PTS + FG)
summary(model_olm) # Only PTS is significant s
```
*Insert stuff about OLM kitchen sink here*

We also ran a bootstrap on the kitchen sink OLM to see if the relationship between the predictor variables is or is not impacted by bootstrapping. We ran the bootstrap with 1000 repetitions, getting a histogram for the t statistic for each OLM coefficient.

```{r bootstrap, warning=FALSE, message=FALSE}
set.seed(1109)


# function to obtain regression weights
bs <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample
  fit <- lm(formula, data=d)
  return(coef(fit))
}
# bootstrapping with 1000 replications
results <- boot(data = nbaper36select, statistic=bs,
   R=1000, formula = MP ~ TRB + AST + TOV + FTA + PTS + FG)

# view results
tvalues = as.data.frame(results[2])
tnames = colnames(nbaper36select)[c(-1, -2)]
colnames(tvalues) = c("Intercept", tnames)
origtvalues = as.numeric(unlist(results$t0))

tplot1 = ggplot(data = tvalues, aes(x = Intercept)) + geom_histogram() + geom_vline(xintercept = origtvalues[1])
tplot2 = ggplot(data = tvalues, aes(x = TRB)) + geom_histogram() + geom_vline(xintercept = origtvalues[2])
tplot3 = ggplot(data = tvalues, aes(x = AST)) + geom_histogram() + geom_vline(xintercept = origtvalues[3])
tplot4 = ggplot(data = tvalues, aes(x = TOV)) + geom_histogram() + geom_vline(xintercept = origtvalues[4])
tplot5 = ggplot(data = tvalues, aes(x = FTA)) + geom_histogram() + geom_vline(xintercept = origtvalues[5])
tplot6 = ggplot(data = tvalues, aes(x = PTS)) + geom_histogram() + geom_vline(xintercept = origtvalues[6])
tplot7 = ggplot(data = tvalues, aes(x = FG)) + geom_histogram() + geom_vline(xintercept = origtvalues[7])

grid.arrange(tplot1, tplot2, tplot3, tplot4, tplot5, tplot6, tplot7, ncol = 2)
```

<<<<<<< HEAD
```{r}
#OLM
model_olm = lm(data = nbaper36select, MP ~ TRB + AST + TOV + FTA + PTS + FG)
summary(model_olm) # Only PTS is significant s
```
=======
From this, we can see that the t-statistic for `TRB`, `FG`, and `FTA` are not indicative of predictive value, white the t-statistic for `PTS`, `AST`, and `TOV` are.
>>>>>>> 00e37acb971d56c45256ce402fa3702e22ca4bab


## Full OLS Multiple Regression Test
$H_0: \beta_1 = \beta_2 =\beta_3 =\beta_4 =\beta_5 =\beta_6 =0$

$H_A: \exists \beta_j \ni [\beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6] s.t. \beta_j \neq 0$

Where $\beta_1$ is the effect of points on minutes played, $\beta_2$ is the effect of turnovers on minutes played,  $\beta_3$ is the effect of field goal percentage on minutes played, $\beta_4$ is the effect of total rebounds on minutes played, $\beta_5$ is the effect of assists on minutes played, and  $\beta_6$ is the effect of free throw attemps on minutes played.

We will be using a significance level of $\alpha = 0.05$

```{r}
#Residual Plot
mplot(model_olm, which = c(1))
```
Conditions:
As illustrated by the residual vs fitted plot, the residuals are approximately normally distributed as there is random scatter throughout the distribution. The plot all illuysrates the condition of equal variance being met as the variance is quite variable throughout the entirety of the data. We can also say that the data are independent of one another as one player's statistics are not influenced by that of another player.

The F- Statistic is 32.36 on 6 and 342 degrees of freedom, with an assocaited p-value less than 0.0001, so at the signficance level of $\alpha = 0.05$, the model is singifiance and we have significant evidence to reject the $H_0$ and claimed that $\exists \beta_j \ni [\beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6] s.t. \beta_j \neq 0$


## Reduced OLS Multiple Regression Test
$H_0: \beta_1 = \beta_2 =\beta_3 =0$

$H_A: \exists \beta_j \ni [\beta_1, \beta_2, \beta_3] s.t. \beta_j \neq 0$

Where $\beta_1$ is the effect of points on minutes played, $\beta_2$ is the effect of turnovers on minutes played,  and $\beta_3$ is the effect of   assists on minutes played.

We will be using a significance level of $\alpha = 0.05$

```{r}
ols_mod1 <- lm(MP ~   AST + TOV  + PTS , data = nbaper36select)
summary(ols_mod1)
```


```{r}
mplot(ols_mod1, which = c(1))
```
Conditions:
As illustrated by the residual vs fitted plot, the residuals are approximately normally distributed as there is random scatter throughout the distribution. The plot all illuysrates the condition of equal variance being met as the variance is quite variable throughout the entirety of the data. We can also say that the data are independent of one another as one player's statistics are not influenced by that of another player.


The F- Statistic is 64.3 on 3 and 345 degrees of freedom, with an assocaited p-value less than 0.0001, so at the signficance level of $\alpha = 0.05$, the model is singifiance and we have significant evidence to reject the $H_0$ and claimed that $\exists \beta_j \ni [\beta_1, \beta_2, \beta_3] s.t. \beta_j \neq 0$.



## Full JHM Model
$H_0: \beta_1 = \beta_2 =\beta_3 =\beta_4 =\beta_5 =\beta_6 =0$

$H_A: \exists \beta_j \ni [\beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6] s.t. \beta_j \neq 0$

Where $\beta_1$ is the effect of points on minutes played, $\beta_2$ is the effect of turnovers on minutes played,  $\beta_3$ is the effect of field goal percentage on minutes played, $\beta_4$ is the effect of total rebounds on minutes played, $\beta_5$ is the effect of assists on minutes played, and  $\beta_6$ is the effect of free throw attemps on minutes played.

We will be using a significance level of $\alpha = 0.05$
```{r}
model_f = rfit(data = nbaper36select, MP ~ TRB + AST + TOV + FTA + PTS + FG)
summary(model_f)
```
Conditions: There are no conditions for the nonparametric JHM hypothesis test, aside from the fact that the data are independent. We can say this condition is met as one player's statistics are not influenced by others.

The Wald Statistic has a value of 188.9863 with an associated p-value of esentially 0, so we have significant evidence to reject the $H_0$ and say that $\exists \beta_j \ni [\beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6] s.t. \beta_j \neq 0$. 



## Reduced JHM Model

$H_0: \beta_1 = \beta_2 =\beta_3 =0$

$H_A: \exists \beta_j \ni [\beta_1, \beta_2, \beta_3] s.t. \beta_j \neq 0$

Where $\beta_1$ is the effect of points on minutes played, $\beta_2$ is the effect of turnovers on minutes played,  and $\beta_3$ is the effect of   assists on minutes played.

We will be using a significance level of $\alpha = 0.05$

```{r}
model_f1 = rfit(data = nbaper36select, MP ~ AST + TOV + PTS)
summary(model_f1)
```
Conditions: There are no conditions for the nonparametric JHM hypothesis test, aside from the fact that the data are independent. We can say this condition is met as one player's statistics are not influenced by others.


The Wald Statistic has a value of 188.395 with an associated p-value of esentially 0, so we have significant evidence to reject the $H_0$ and say that 
$\exists \beta_j \ni [\beta_1, \beta_2, \beta_3] s.t. \beta_j \neq 0$.


##JHM
```{r}
# JHM
model_f = rfit(data = nbaper36select, MP ~ TRB + BLK + TOV + FTA + PTS + FG)
summary(model_f)$coefficients
simp_mod <- rfit(MP ~ PTS, data = nbaper36select)
#drop.test(model_f, simp_mod)

part_mod <- rfit(MP~ PTS + TRB, data=nbaper36select)
drop.test(model_f, part_mod)
```

Super high p value here... probably can drop everything except TRB and PTS.


```{r}
#ols
ols_mod <- lm(MP ~ TRB + BLK + TOV + FTA + PTS + FG, data = nbaper36select)
summary(ols_mod)
```

```{r}
#reb spline
simp_reb <- lm(MP ~ TRB, data = nbaper36select)
s_reb <- gam(MP ~ s(TRB), data = nbaper36select)
AIC(simp_reb) 
AIC(s_reb) #slightly lower aic
```

```{r}
#ast spline
simp_ast <- lm(MP ~ AST, data = nbaper36select)
s_ast <- gam(MP ~ s(AST), data = nbaper36select)
AIC(simp_ast) #lower aic
AIC(s_ast) 
```

```{r}
#tov spline
simp_tov <- lm(MP ~ TOV, data = nbaper36select)
s_tov <- gam(MP ~ s(TOV), data = nbaper36select)
AIC(simp_tov) #slightly lower aic
AIC(s_tov)
```

```{r}
#fta spline
simp_fta <- lm(MP ~ FTA, data = nbaper36select)
s_fta <- gam(MP ~ s(FTA), data = nbaper36select)
AIC(simp_fta) #slightly lower aic
AIC(s_fta)
```

```{r}
#pts spline
simp_pts <- lm(MP ~ PTS, data = nbaper36select)
s_pts <- gam(MP ~ s(PTS), data = nbaper36select)
AIC(simp_pts) 
AIC(s_pts) #slightly lower aic
```

```{r}
#fg spline
simp_fg <- lm(MP ~ FG, data = nbaper36select)
s_fg <- gam(MP ~ s(FG), data = nbaper36select)
AIC(simp_fg) 
AIC(s_fg) #lower aic
```

```{r}
# OLS KS Test Residual Plot
mplot(ols_mod1, which = c(1))

```
```{r}
#OLS KS Code
ks.test(x = resid(ols_mod), y = pnorm, mean = 0, sd = 5.796699)
```
$H_0:$ F(t) = G(t) for all t.

$H_A:$ F(t) $\neq$ G(t) for at least one t.

where F is the normal CDF of mean zero and standard deviation 5.7967 and G is the distribution of the residuals of the reduced ols.

We will be using a significance level of $\alpha = 0.05$

Conditions: There is independence in the distributions and the data comes from a continuous population. For both the normal and residual distribution, we can say they are continuous. We all can assume independence. 

The test-statistic, D-value, generated by the Kolmogorov-Smirnov test is 0.049634 with an associated p-value of 0.3562. At the significance level of $\alpha = 0.05$, this indicates we do not have significant evidence to reject the $H_0$ and claim the residuals follow a roughly normal distribution.


```{r}
jhmresid = ggplot(data = jhmdataframe, aes(x = Fitted_Values, y = Residuals)) +
                    geom_point() + geom_smooth(method = lm, color = "red") + labs(title = "JHM Residual vs Fitted")

plot(jhmresid)
```


$H_0:$ F(t) = G(t) for all t.

$H_A:$ F(t) $\neq$ G(t) for at least one t.

where F is the normal CDF of mean zero and standard deviation 5.7975 and G is the distribution of the residuals of the reduced JHM model

We will be using a significance level of $\alpha = 0.05$

Conditions: There is independence in the distributions and the data comes from a continuous population. For both the normal and residual distribution, we can say they are continuous. We all can assume independence. 

The test-statistic, D-value, generated by the Kolmogorov-Smirnov test is 0.046003 with an associated p-value of 0.4511 At the significance level of $\alpha = 0.05$, this indicates we do not have significant evidence to reject the $H_0$ and claim the residuals follow a roughly normal distribution.

```{r}
#JHM KS Test
ks.test(x = resid(model_f), y = pnorm, mean = 0, sd = 5.797549)
```


```{r}
gamresid = ggplot(data = gamdataframe, aes(x = Fitted_Values, y = Residuals)) +
                    geom_point() + geom_smooth(method = lm, color = "red") + labs(title = "GAM Residual vs Fitted")
plot(gamresid)
```


$H_0:$ F(t) = G(t) for all t.

$H_A:$ F(t) $\neq$ G(t) for at least one t.

where F is the normal CDF of mean zero and standard deviation 5.6249 and G is the distribution of the residuals of the reduced GAM model

We will be using a significance level of $\alpha = 0.05$

Conditions: There is independence in the distributions and the data comes from a continuous population. For both the normal and residual distribution, we can say they are continuous. We all can assume independence. 

The test-statistic, D-value, generated by the Kolmogorov-Smirnov test is 0.050011 with an associated p-value of 0.3472 At the significance level of $\alpha = 0.05$, this indicates we do not have significant evidence to reject the $H_0$ and claim the residuals follow a roughly normal distribution.

```{r}
#Gam KS Test
ks.test(x = resid(full_gam), y = pnorm, mean = 0, sd = 5.624864)

```

```{r}
#gam
full_gam <- gam(MP ~ s(TRB) + AST + TOV + FTA + s(PTS) + s(FG), data = nbaper36select)
full_gam_c = predict(full_gam, type = "terms")
full_gam_y <- fitted(full_gam)
nba_sub <- select(.data = nbaper36select, MP, TRB, AST, TOV, FTA, PTS, FG)

mp_m = mean(nbaper36select$MP)

fullgam_plots <- cbind(nba_sub, full_gam_c, full_gam_y)
cnn <- c(colnames(nba_sub), "trb_pred", "blk_pred", "tov_pred", "fta_pred", "pts_pred", "fg_pred","mp_pred")
colnames(fullgam_plots) <- cnn
```

```{r}
#creating gam plots
plot1 <- ggplot(data = nbaper36select, aes(x = TRB, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = TRB, y = trb_pred + mp_m))

plot2 <- ggplot(data = nbaper36select, aes(x = AST, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = AST, y = blk_pred + mp_m))

plot3 <- ggplot(data = nbaper36select, aes(x = TOV, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = TOV, y = tov_pred + mp_m))

plot4 <- ggplot(data = nbaper36select, aes(x = FTA, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = FTA, y = fta_pred + mp_m))

plot5 <- ggplot(data = nbaper36select, aes(x = PTS, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = PTS, y = pts_pred + mp_m))

plot6 <- ggplot(data = nbaper36select, aes(x = FG, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = FG, y = fg_pred + mp_m))
```

```{r}
#graphing gam

grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, ncol = 2)

```

```{r}
full_gam1 <- gam(MP ~ s(TRB) + AST + TOV + s(PTS) + s(FG), data = nbaper36select)
AIC(full_gam) 
AIC(full_gam1)
```

## Model Fit and CV


We also decided to see the fit statistics for the three models, as well as running a 5-fold crossvalidation on the fit statistics. Using that, we got the following fit statistics.

```{r, message = FALSE, warning = FALSE}
fit_final1 = rbind(fit_ols(ols_mod1),fit_jhm(model_f1),fit_gam(full_gam1))
rownames(fit_final1) =c("OLS", "JHM", "GAM")
kable(round(fit_final1, 4))%>% kable_styling(position = "center")

out10a =cv_rmc(dat = nbaper36select, ols_mod = ols_mod1, jhm_mod = model_f1, gam_mod = full_gam1)
kable(round(out10a,4))%>% kable_styling(position = "center")
```

Overall, all the values indicate low predictive power. Interestingly, the GAM displays higher values in both $R^2$ and $L1_{prop}$ when compared to the OLS and JHM, yet displays a lower value for $R^2_{Adj}$ wheen compared to the other two models. This seems to indicate the GAM is overfit. This issue along with the normality of the residuals as shown in the Kolmogorov-Smirnov test seem to indicate that the OLS model is best suited for prediction.

## Conclusion

In conclusion, it seems that the models we created do an overall poor job of predicting the minutes played of a given NBA player. The different models also all had comparable fit statistics, indicating there wasn't much of a difference in performance between the three models. Future avenues of exploration could include creating position-specific models, using other statistics including defensive statistics or other advanced statistics, changing the filter conditions to include more or fewer players, or using multiple seasons worth of data. 