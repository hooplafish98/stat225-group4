---
title: "Predicting Minutes Played In The NBA"
output:
  beamer_presentation: default
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
require(mosaic)   # Load additional packages here 
library(readr)
library(boot) #for bootstrap
library(Rfit)
library(ggplot2)
library(gam)
library(splines)
library(gridExtra)
library(knitr)
library(kableExtra)
library(stringr)
options(scipen = 999)
```

```{r, include=FALSE}
#loaded the data here... not sure if we need to show this on a slide or not

#Fit setup
#Fit statistics for OLS

fit_ols = function(model) {
  yy = model$residuals + model$fitted.values
  rsq = 1- sum(model$residuals^2)/sum((yy- mean(yy))^2)
  nn =length(yy)
  adjrsq = 1-(1-rsq)*((nn-1)/(nn- length(model$coefficients)))
  propL1 = 1- sum(abs(model$residuals))/sum(abs(yy- mean(yy)))
  
  return(cbind(rsq = rsq, adjrsq = adjrsq, propL1 = propL1))
  }

#Fit statistics for JHM

fit_jhm = function(model) {
  rsq = 1- sum(model$residuals^2)/sum((model$y- mean(model$y))^2)
  nn =length(model$y)
  adjrsq = 1-(1-rsq)*((nn-1)/(nn- length(model$coefficients)))
  propL1 = 1- sum(abs(model$residuals))/sum(abs(model$y- mean(model$y)))
  
  return(cbind(rsq = rsq, adjrsq = adjrsq, propL1 = propL1))
  }

#Fit statistics for GAM

fit_gam = function(model) {
  rsq = 1-model$deviance/model$null.deviance
  adjrsq = 1-(1-rsq)*(model$df.null/model$df.residual)
  propL1 = 1- sum(abs(model$residuals))/sum(abs(model$y- mean(model$y)))
  
  return(cbind(rsq = rsq, adjrsq = adjrsq, propL1 = propL1))}

#CV setup

#General fit statistics

fit_gen = function(y, res, df){
  rsq = 1- sum(res^2)/sum((y- mean(y))^2)
  nn =length(y)
  adjrsq = 1-(1-rsq)*((nn-1)/(nn-df))
  propL1 = 1- sum(abs(res))/sum(abs(y- mean(y)))
  
  return(cbind(rsq = rsq, adjrsq = adjrsq, propL1 = propL1))
  }

#My cross-validation function for this project

cv_rmc = function(dat, ols_mod, jhm_mod, gam_mod, k = 5, m = 10){
  
  #(Some) error checking
  
  if(class(ols_mod)!="lm")stop('ols_mod should come from the lm() function')
  if(class(jhm_mod)!="rfit")stop('jhm_mod should come from the rfit() function')
  if(class(gam_mod)[1]!="Gam")stop('gam_mod should come from the gam() function')
  #Create model call character strings with subsetted data; uses stringr f()s
  dat.name =paste0("data = ",deparse(substitute(dat)))
  ols_call =capture.output(ols_mod$call)
  ols_call =str_replace(ols_call, dat.name, "data = dat[-part[[i]], ]")
  jhm_call =capture.output(jhm_mod$call)
  jhm_call =str_replace(jhm_call, dat.name, "data = dat[-part[[i]], ]")
  gam_call =paste(str_trim(capture.output(gam_mod$call)), sep="", collapse="")
  gam_call =str_replace(gam_call, dat.name, "data = dat[-part[[i]], ]")
  #Set up objects
  ols_fit =matrix(nrow = m, ncol = 3)
  jhm_fit = ols_fit; gam_fit = ols_fit
  yy = jhm_mod$y
  nn =dim(as.data.frame(dat))[1]
  oos_lmres =vector(length = nn)
  oos_jhres = oos_lmres; oos_gares = oos_lmres
  df_ols =length(ols_mod$coefficients)
  df_jhm =length(jhm_mod$coefficients)
  df_gam = nn-gam_mod$df.residual
  
  #Repeat k-fold cross-validation m times
  for(j in 1:m) {
    #Split data into k equal-ish parts, with random indices
    part =suppressWarnings(split(sample(nn), 1:k))
    #Execute model calls for all k folds; %*% is matrix multiplication
    for(i in 1:k){
      lm_mod =eval(parse(text = ols_call))
      pred =predict(object = lm_mod, newdata = dat[part[[i]],])
      oos_lmres[part[[i]]] = yy[part[[i]]]-pred
      jh_mod =eval(parse(text = jhm_call))
      subdat =select(.data = dat,colnames(jh_mod$x)[-1])[part[[i]],]
      subdat =cbind(1,as.matrix.data.frame(subdat))
      pred = subdat%*%jh_mod$coefficients
      oos_jhres[part[[i]]] = yy[part[[i]]]-pred
      
      ga_mod =eval(parse(text = gam_call))
      pred =predict(object = ga_mod, newdata = dat[part[[i]],])
      oos_gares[part[[i]]] = yy[part[[i]]]-pred
      }
    ols_fit[j, ] =fit_gen(y = yy, res = oos_lmres, df = df_ols)
    jhm_fit[j, ] =fit_gen(y = yy, res = oos_jhres, df = df_jhm)
    gam_fit[j, ] =fit_gen(y = yy, res = oos_gares, df = df_gam)}
  #Manage output -- average fit statistics
  outtie = rbind(colMeans(ols_fit),colMeans(jhm_fit),colMeans(gam_fit))
  colnames(outtie) = paste0("cv.",colnames(fit_ols(lm_mod)))
  row.names(outtie) =c("OLS", "JHM", "GAM")
  
  return(outtie)
  }

#Setup for everything else

URL = "https://www.amherst.edu/system/files/media/SP20_S06_Group4_data.csv"
nbaper36 <- read_csv(URL) #reads in csv #reads in csv
totplayers = as.list(nbaper36 %>% filter(Tm == "TOT") %>% select(Player))#list of players on multiple teams in season
nbaper36 = nbaper36 %>% mutate(MP = MP/G) %>% filter(G > 41) %>% filter(!(Player %in% totplayers$Player & Tm != "TOT")) #mutates MP to reflect minutes played per game, filters for players who played >41 games, and removes player entries that are not season totals
nbaper36select = select(nbaper36, Player, MP, TRB, AST, TOV, FTA, PTS, 'FG%') %>% rename(FG = 'FG%')

GGally::ggpairs(select(nbaper36select, -Player))
```


## Background & Motivations

- We hope to predict minutes played through other types of basketball statistics.
- Minutes played is valuable to keep track of because more minutes played will offer a player more opportunities to contribute to the game.
- We looked at statistics from the 2018-2019 NBA season for players who played in at least 41 games, or half the season.
- We converted counting stats (eg rebounds) into per 36 minute rates. These rates can provide a good insight into a playerâ€™s productivity without interference from the number of minutes played by the player.


## Kernel Density Estimation

```{r kde}
kde1 = ggplot(data = nbaper36select, aes(MP)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde2 = ggplot(data = nbaper36select, aes(TRB)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde3 = ggplot(data = nbaper36select, aes(AST)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde4 = ggplot(data = nbaper36select, aes(TOV)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde5 = ggplot(data = nbaper36select, aes(FTA)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde6 = ggplot(data = nbaper36select, aes(PTS)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde7 = ggplot(data = nbaper36select, aes(FG)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)

grid.arrange(kde1, kde2, kde3, kde4, kde5, kde6, kde7, ncol = 3)
```

## Scatterplots

```{r}
splot1 <- ggplot(data = nbaper36select, aes(x = TRB, y = MP)) + 
  geom_point()

splot2 <- ggplot(data = nbaper36select, aes(x = AST, y = MP)) + 
  geom_point()
splot3 <- ggplot(data = nbaper36select, aes(x = TOV, y = MP)) + 
  geom_point()
splot4 <- ggplot(data = nbaper36select, aes(x = FTA, y = MP)) + 
  geom_point()
splot5 <- ggplot(data = nbaper36select, aes(x = PTS, y = MP)) + 
  geom_point()
splot6 <- ggplot(data = nbaper36select, aes(x = FG, y = MP)) + 
  geom_point()

grid.arrange(splot1, splot2, splot3, splot4, splot5, splot6, ncol = 2)
```


## OLS Multiple Linear Regression

```{r ols}
ols_mod <- lm(MP ~ TRB + AST + TOV + FTA + PTS + FG, data = nbaper36select)
tableols = summary(ols_mod)$coefficients[,c(1,4)]

kable(round(tableols,4), col.names = c("Predictor", "P-Value"))%>% kable_styling(position = "center")

```

MP = 9.573 - 0.052 TRB + 1.185 AST - 1.884 TOV
- 0.312 FTA + 0.963 PTS - 0.924 FG

## Bootstrap

```{r bootstrap, warning=FALSE, message=FALSE}
set.seed(1109)


# function to obtain regression weights
bs <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample
  fit <- lm(formula, data=d)
  return(coef(fit))
}
# bootstrapping with 1000 replications
results <- boot(data = nbaper36select, statistic=bs,
   R=1000, formula = MP ~ TRB + AST + TOV + FTA + PTS + FG)

# view results
tvalues = as.data.frame(results[2])
tnames = colnames(nbaper36select)[c(-1, -2)]
colnames(tvalues) = c("Intercept", tnames)
origtvalues = as.numeric(unlist(results$t0))

tplot1 = ggplot(data = tvalues, aes(x = Intercept)) + geom_histogram() + geom_vline(xintercept = origtvalues[1])
tplot2 = ggplot(data = tvalues, aes(x = TRB)) + geom_histogram() + geom_vline(xintercept = origtvalues[2])
tplot3 = ggplot(data = tvalues, aes(x = AST)) + geom_histogram() + geom_vline(xintercept = origtvalues[3])
tplot4 = ggplot(data = tvalues, aes(x = TOV)) + geom_histogram() + geom_vline(xintercept = origtvalues[4])
tplot5 = ggplot(data = tvalues, aes(x = FTA)) + geom_histogram() + geom_vline(xintercept = origtvalues[5])
tplot6 = ggplot(data = tvalues, aes(x = PTS)) + geom_histogram() + geom_vline(xintercept = origtvalues[6])
tplot7 = ggplot(data = tvalues, aes(x = FG)) + geom_histogram() + geom_vline(xintercept = origtvalues[7])

grid.arrange(tplot1, tplot2, tplot3, tplot4, tplot5, tplot6, tplot7, ncol = 2)
```


## OLS Multiple Linear Regression

```{r ols1}

ols_mod1 <- lm(MP ~   AST + TOV  + PTS , data = nbaper36select)
tableols_r = summary(ols_mod1)$coefficients[,c(1,4)]

kable(round(tableols_r,4), col.names = c("Predictor", "P-Value"))%>% kable_styling(position = "center")
``` 

MP = 9.3395 + 1.2937 AST - 2.3198 TOV +
0.8941 PTS

## JHM Multiple Regression

```{r jhm}
model_f = rfit(data = nbaper36select, MP ~ TRB + AST + TOV + FTA + PTS + FG)
tablejhm= summary(model_f)$coefficients [,c(1,4)]

kable(round(tablejhm,4), col.names = c("Predictor", "P-Value"))%>% kable_styling(position = "center")

```

MP = 9.294 - 0.090 TRB + 1.176 AST - 1.764 TOV
- 0.365 FTA + 0.987 PTS + 0.341 FG

## JHM Multiple Regression

```{r jhm1}
model_f1 = rfit(data = nbaper36select, MP ~ AST + TOV + PTS)
tablejhm_r= summary(model_f1)$coefficients [,c(1,4)]

kable(round(tablejhm_r,4), col.names = c("Predictor", "P-Value"))%>% kable_styling(position = "center")
```

MP = 9.4239 + 1.2996 AST - 2.272 TOV +
0.9083 PTS 

## Generalized Additive Model

```{r, include = FALSE}
#reb spline
s_reb <- gam(MP ~ s(TRB), data = nbaper36select)
rebplot = data.frame(nbaper36select$TRB, fitted(s_reb))
colnames(rebplot) = c("TRB", "smooth")

#AST spline
simp_ast <- gam(MP ~ AST, data = nbaper36select)
astplot = data.frame(nbaper36select$AST, fitted(simp_ast))
colnames(astplot) = c("AST", "simp")

#tov spline
simp_tov <- lm(MP ~ TOV, data = nbaper36select)
tovplot = data.frame(nbaper36select$TOV, fitted(simp_tov))
colnames(tovplot) = c("TOV", "simp")

#fta spline
simp_fta <- lm(MP ~ FTA, data = nbaper36select)
ftaplot = data.frame(nbaper36select$FTA, fitted(simp_fta))
colnames(ftaplot) = c("FTA", "simp")

#pts spline
s_pts <- gam(MP ~ s(PTS), data = nbaper36select)
ptsplot = data.frame(nbaper36select$PTS, fitted(s_pts))
colnames(ptsplot) = c("PTS", "smooth")

#fg spline
s_fg <- gam(MP ~ s(FG), data = nbaper36select)
fgplot = data.frame(nbaper36select$FG, fitted(s_fg))
colnames(fgplot) = c("FG", "smooth")
```

```{r, include = FALSE}
#gam
full_gam <- gam(MP ~ s(TRB) + AST + TOV + FTA + s(PTS) + s(FG), data = nbaper36select)
full_gam_c = predict(full_gam, type = "terms")
full_gam_y <- fitted(full_gam)
nba_sub <- select(.data = nbaper36select, MP, TRB, AST, TOV, FTA, PTS, FG)

mp_m = mean(nbaper36select$MP)

fullgam_plots <- cbind(nba_sub, full_gam_c, full_gam_y)
cnn <- c(colnames(nba_sub), "trb_pred", "AST_pred", "tov_pred", "fta_pred", "pts_pred", "fg_pred","mp_pred")
colnames(fullgam_plots) <- cnn

fullgam_plots <- cbind(fullgam_plots, 
                       best_trb = rebplot$smooth,
                       best_ast = astplot$simp,
                       best_tov = tovplot$simp,
                       best_fta = ftaplot$simp,
                       best_pts = ptsplot$smooth,
                       best_fg = fgplot$smooth)
```

```{r, include = FALSE}
#creating gam plots
plot1 <- ggplot(data = nbaper36select, aes(x = TRB, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "red", data = fullgam_plots,
            aes(x = TRB, y = best_trb)) +
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = TRB, y = trb_pred + mp_m))

plot2 <- ggplot(data = nbaper36select, aes(x = AST, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "red", data = fullgam_plots,
            aes(x = AST, y = best_ast)) +
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = AST, y = AST_pred + mp_m))

plot3 <- ggplot(data = nbaper36select, aes(x = TOV, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "red", data = fullgam_plots,
            aes(x = TOV, y = best_tov)) +
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = TOV, y = tov_pred + mp_m))

plot4 <- ggplot(data = nbaper36select, aes(x = FTA, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "red", data = fullgam_plots,
            aes(x = FTA, y = best_fta)) +
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = FTA, y = fta_pred + mp_m))

plot5 <- ggplot(data = nbaper36select, aes(x = PTS, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "red", data = fullgam_plots,
            aes(x = PTS, y = best_pts)) +
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = PTS, y = pts_pred + mp_m))

plot6 <- ggplot(data = nbaper36select, aes(x = FG, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "red", data = fullgam_plots,
            aes(x = FG, y = best_fg)) +
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = FG, y = fg_pred + mp_m))
```

```{r}
#graphing gam
grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, ncol = 2)

#AIC(full_gam)
```

AIC: `r AIC(full_gam)`


## Generalized Additive Model

```{r, include = FALSE}
#gam1
full_gam1 <- gam(MP ~ AST + TOV + s(PTS) + s(FG), data = nbaper36select)
full_gam_c1 = predict(full_gam1, type = "terms")
full_gam_y1 <- fitted(full_gam1)

fullgam_plots1 <- cbind(nba_sub, full_gam_c1, full_gam_y1)
cnn1 <- c(colnames(nba_sub), "AST_pred", "tov_pred", "pts_pred", "fg_pred","mp_pred")
colnames(fullgam_plots1) <- cnn1

fullgam_plots1 <- cbind(fullgam_plots1, 
                       best_ast = astplot$simp,
                       best_tov = tovplot$simp,
                       best_pts = ptsplot$smooth,
                       best_fg = fgplot$smooth)


#creating gam plots

plot7 <- ggplot(data = nbaper36select, aes(x = AST, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "red", data = fullgam_plots1,
            aes(x = AST, y = best_ast)) +
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots1,
            aes(x = AST, y = AST_pred + mp_m))

plot8 <- ggplot(data = nbaper36select, aes(x = TOV, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "red", data = fullgam_plots1,
            aes(x = TOV, y = best_tov)) +
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots1,
            aes(x = TOV, y = tov_pred + mp_m))

plot9 <- ggplot(data = nbaper36select, aes(x = PTS, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "red", data = fullgam_plots1,
            aes(x = PTS, y = best_pts)) +
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots1,
            aes(x = PTS, y = pts_pred + mp_m))

plot10 <- ggplot(data = nbaper36select, aes(x = FG, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "red", data = fullgam_plots1,
            aes(x = FG, y = best_fg)) +
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots1,
            aes(x = FG, y = fg_pred + mp_m))
```

```{r}
grid.arrange(plot7, plot8, plot9, plot10, ncol=2)

#AIC(full_gam1)
```

AIC: `r AIC(full_gam1)`



## Residual Plots
```{r ks test, include= FALSE}

jhmdataframe = data.frame(model_f1$residuals, model_f1$fitted.values)
colnames(jhmdataframe) = c("Residuals", "Fitted_Values")

olmdataframe = data.frame(ols_mod1$residuals, ols_mod1$fitted.values)
colnames(olmdataframe) = c("Residuals", "Fitted_Values")

gamdataframe = data.frame(full_gam1$residuals, full_gam1$fitted.values)
colnames(gamdataframe) = c("Residuals", "Fitted_Values")


olmresid = ggplot(data = olmdataframe, aes(x = Fitted_Values, y = Residuals)) +
                    geom_point() + geom_smooth(method = lm, color = "red", width = "0.1") + labs(title = "OLM Residual vs Fitted")

jhmresid = ggplot(data = jhmdataframe, aes(x = Fitted_Values, y = Residuals)) +
                    geom_point() + geom_smooth(method = lm, color = "red", width = "0.1") + labs(title = "JHM Residual vs Fitted")

gamresid = ggplot(data = gamdataframe, aes(x = Fitted_Values, y = Residuals)) +
                    geom_point() + geom_smooth(method = lm, color = "red", width = "0.1") + labs(title = "GAM Residual vs Fitted")
```

```{r}
grid.arrange(olmresid, jhmresid, gamresid, ncol = 2)
```



## Kolmogorov Smirnov Test
```{r}
#OLS

olsks_t = rbind(ks.test(x = resid(ols_mod1), y = pnorm, mean = 0, sd = 5.96654)$p.value, ks.test(x = resid(model_f1), y = pnorm, mean = 0, sd = 5.968016)$p.value, ks.test(x = resid(model_f1), y = pnorm, mean = 0, sd = 5.968016)$p.value)

rownames(olsks_t) =c("OLS", "JHM", "GAM")
kable(round(olsks_t,4))%>% kable_styling(position = "center")


```

Not enough evidence to say the residual distributions stray from Normal.


```{r, include = FALSE} 
# get 95% confidence intervals
boot.ci(results, type="bca", index=1) # intercept
boot.ci(results, type="bca", index=2) # TRB
boot.ci(results, type="bca", index=3) # AST
boot.ci(results, type="bca", index=4) # TOV
boot.ci(results, type="bca", index=5) # FTA
boot.ci(results, type="bca", index=6) # PTS
boot.ci(results, type="bca", index=7) # FG%
```


## Model Fit CV (Both Models)

```{r, message = FALSE, warning = FALSE}
out10 =cv_rmc(dat = nbaper36select, ols_mod = ols_mod, jhm_mod = model_f, gam_mod = full_gam)
kable(round(out10,4))%>% kable_styling(position = "center")

out10a =cv_rmc(dat = nbaper36select, ols_mod = ols_mod1, jhm_mod = model_f1, gam_mod = full_gam1)
kable(round(out10a,4))%>% kable_styling(position = "center")
```
## Model Fit and CV (Reduced Models)

```{r, message = FALSE, warning = FALSE}
fit_final1 = rbind(fit_ols(ols_mod1),fit_jhm(model_f1),fit_gam(full_gam1))
rownames(fit_final1) =c("OLS", "JHM", "GAM")
kable(round(fit_final1, 4))%>% kable_styling(position = "center")

out10a =cv_rmc(dat = nbaper36select, ols_mod = ols_mod1, jhm_mod = model_f1, gam_mod = full_gam1)
kable(round(out10a,4))%>% kable_styling(position = "center")
```

## Takeaways

* Raw box score numbers do not do a good job of predicting the number of minutes played

* The models tested seem to have similar values for $R^2$, $Adj R^2$, and $L1_{prop}$.

* Future studies could examine different models for different positions, the usage of other stats, or different filtering conditions