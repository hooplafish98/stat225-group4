---
title: "Presentation"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
require(mosaic)   # Load additional packages here 
library(readr)
library(boot) #for bootstrap
library(Rfit)
library(ggplot2)
library(gam)
library(splines)
library(gridExtra)
```

```{r, include=FALSE}
#loaded the data here... not sure if we need to show this on a slide or not


nbaper36 <- read_csv("nbaper36.csv") #reads in csv
totplayers = as.list(nbaper36 %>% filter(Tm == "TOT") %>% select(Player))#list of players on multiple teams in season
nbaper36 = nbaper36 %>% mutate(MP = MP/G) %>% filter(G > 41) %>% filter(!(Player %in% totplayers$Player & Tm != "TOT")) #mutates MP to reflect minutes played per game, filters for players who played >41 games, and removes player entries that are not season totals
nbaper36select = select(nbaper36, Player, MP, TRB, AST, TOV, FTA, PTS, 'FG%') %>% rename(FG = 'FG%')

GGally::ggpairs(select(nbaper36select, -Player))
```


## Background/Motivations

- We hope to predict minutes played through other types of basketball statistics.
- Minutes played is valuable to keep track of because more minutes played will offer a player more opportunities to contribute to the game.
- We looked at statistics from the 2018-2019 NBA season for players who played in at least 41 games, or half the season.
- We converted counting stats (eg rebounds) into per 36 minute rates. These rates can provide a good insight into a playerâ€™s productivity without interference from the number of minutes played by the player.


## Kernel Density Estimation

```{r kde}
kde1 = ggplot(data = nbaper36select, aes(MP)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde2 = ggplot(data = nbaper36select, aes(TRB)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde3 = ggplot(data = nbaper36select, aes(AST)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde4 = ggplot(data = nbaper36select, aes(TOV)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde5 = ggplot(data = nbaper36select, aes(FTA)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde6 = ggplot(data = nbaper36select, aes(PTS)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)


kde7 = ggplot(data = nbaper36select, aes(FG)) +
  geom_density(bw = "SJ", kernel = "epanechnikov", size = 2)

grid.arrange(kde1, kde2, kde3, kde4, kde5, kde6, kde7, ncol = 3)
```


## OLS multiple linear regression

```{r ols}
ols_mod <- lm(MP ~ TRB + AST + TOV + FTA + PTS + FG, data = nbaper36select)
summary(ols_mod)$coefficients
```

MP = 9.573 - 0.052 TRB + 1.185 AST - 1.884 TOV
- 0.312 FTA + 0.963 PTS - 0.924 FG


## JHM multiple regression

```{r jhm}
model_f = rfit(data = nbaper36select, MP ~ TRB + AST + TOV + FTA + PTS + FG)
summary(model_f)$coefficients
```

MP = 9.294 - 0.090 TRB + 1.176 AST - 1.764 TOV
- 0.365 FTA + 0.987 PTS + 0.341 FG


## Generalized additive model

```{r, include = FALSE}
#reb spline
s_reb <- gam(MP ~ s(TRB), data = nbaper36select)

#AST spline
s_AST <- gam(MP ~ s(AST), data = nbaper36select)

#tov spline
simp_tov <- lm(MP ~ TOV, data = nbaper36select)

#fta spline
simp_fta <- lm(MP ~ FTA, data = nbaper36select)

#pts spline
s_pts <- gam(MP ~ s(PTS), data = nbaper36select)

#fg spline
s_fg <- gam(MP ~ s(FG), data = nbaper36select)
```

```{r, include = FALSE}
#gam
full_gam <- gam(MP ~ s(TRB) + s(AST) + TOV + FTA + s(PTS) + s(FG), data = nbaper36select)
full_gam_c = predict(full_gam, type = "terms")
full_gam_y <- fitted(full_gam)
nba_sub <- select(.data = nbaper36select, MP, TRB, AST, TOV, FTA, PTS, FG)

mp_m = mean(nbaper36select$MP)

fullgam_plots <- cbind(nba_sub, full_gam_c, full_gam_y)
cnn <- c(colnames(nba_sub), "trb_pred", "AST_pred", "tov_pred", "fta_pred", "pts_pred", "fg_pred","mp_pred")
colnames(fullgam_plots) <- cnn
```

```{r, include = FALSE}
#creating gam plots
plot1 <- ggplot(data = nbaper36select, aes(x = TRB, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = TRB, y = trb_pred + mp_m))

plot2 <- ggplot(data = nbaper36select, aes(x = AST, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = AST, y = AST_pred + mp_m))

plot3 <- ggplot(data = nbaper36select, aes(x = TOV, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = TOV, y = tov_pred + mp_m))

plot4 <- ggplot(data = nbaper36select, aes(x = FTA, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = FTA, y = fta_pred + mp_m))

plot5 <- ggplot(data = nbaper36select, aes(x = PTS, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = PTS, y = pts_pred + mp_m))

plot6 <- ggplot(data = nbaper36select, aes(x = FG, y = MP)) + 
  geom_point() + 
  geom_hline(yintercept = mp_m, linetype = 2, color = "blue") + 
  geom_line(inherit.aes = F, size = 2, color = "gold", data = fullgam_plots,
            aes(x = FG, y = fg_pred + mp_m))
```

```{r}
#graphing gam
grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, ncol = 2)

AIC(full_gam) 
```


## Kolmogorov Smirnov Test

```{r ks test}
#sd(resid(ols_mod))

ks.test(x = resid(ols_mod), y = pnorm, mean = 0, sd = 5.96654)

#JHM
#sd(resid(model_f))

ks.test(x = resid(model_f), y = pnorm, mean = 0, sd = 5.968016)
```

## Kolmogorov Smirnov Test

```{r}
#sd(resid(full_gam))

ks.test(x = resid(full_gam), y = pnorm, mean = 0, sd = 5.670123)

```


## Bootstrap

```{r bootstrap}
set.seed(1109)


# function to obtain regression weights
bs <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample
  fit <- lm(formula, data=d)
  return(coef(fit))
}
# bootstrapping with 1000 replications
results <- boot(data = nbaper36select, statistic=bs,
   R=1000, formula = MP ~ TRB + AST + TOV + FTA + PTS + FG)

# view results
boot1 = plot(results, index=1) # intercept
#boot2 = plot(results, index=2) # TRB
#boot3 = plot(results, index=3) # AST
#boot4 = plot(results, index=4) # TOV
#boot5 = plot(results, index=5) # FTA
#boot6 = plot(results, index=6) # PTS
#boot7 = plot(results, index=7) # FG%


# get 95% confidence intervals
boot.ci(results, type="bca", index=1) # intercept
boot.ci(results, type="bca", index=2) # TRB
boot.ci(results, type="bca", index=3) # AST
boot.ci(results, type="bca", index=4) # TOV
boot.ci(results, type="bca", index=5) # FTA
boot.ci(results, type="bca", index=6) # PTS
boot.ci(results, type="bca", index=7) # FG%
```


## Takeaways

* Raw box score numbers do not do a good job of predicting the number of minutes played

* Of the models tested, the GAM seems to do best, but may be because of overfitting

* Future studies could examine different models for different positions, or the usage of other stats